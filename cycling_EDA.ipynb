{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd49762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from scipy import stats\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import requests\n",
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff491e33",
   "metadata": {},
   "source": [
    "# Initial step to examine structure and cleanliness of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804b47db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"data\\\\143JourneyDataExtract02Jan2019-08Jan2019.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83728aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea77286",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2ec0b0",
   "metadata": {},
   "source": [
    "## Initial questions and tasks\n",
    "- convert duration to mins\n",
    "- convert end date and start date to datetime\n",
    "- describe duration- avg journey time per bike\n",
    "- busiest day/time?\n",
    "- chlropleth map of start and end locations\n",
    "- how many unique bikes?\n",
    "- how many journeys per bike/per day?\n",
    "- how many journeys started and ended at the same location?\n",
    "- how many journeys over 30 mins?\n",
    "- above usage trends over the seasons, years, weekdays vs weekends etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d200606",
   "metadata": {},
   "source": [
    "# Download all the relevant files from 2019-21\n",
    "Summary of cells below\n",
    "1. Function defined to scrape urls from TFL website\n",
    "2. Function defined to filter scraped urls by year\n",
    "3. Function defined to read a single csv from a given url\n",
    "4. Function defined to read the relevant csvs for a given year and download the data using 10 threads into a parquet file for efficient storage of ~10M records per year\n",
    "5. Optimise datatypes and combine individual dfs into one and save as parquet for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484727fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tfl_usage_csvs(\n",
    "    url: str = \"https://cycling.data.tfl.gov.uk/#!usage-stats%2F\"\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Scrape TfL cycling usage-stats page using Selenium (handles JavaScript).\n",
    "    Returns list of JourneyDataExtract CSV URLs.\n",
    "    \"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        # Wait for links to load\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.PARTIAL_LINK_TEXT, \"JourneyDataExtract\"))\n",
    "        )\n",
    "        \n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        urls = []\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and \"JourneyDataExtract\" in href and href.endswith(\".csv\"):\n",
    "                urls.append(href)\n",
    "        \n",
    "        if not urls:\n",
    "            raise RuntimeError(\"No JourneyDataExtract CSVs found\")\n",
    "        \n",
    "        return sorted(set(urls))\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d8101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_urls_by_year(csv_urls: list[str], year: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Filter CSV URLs to only include files where the START date matches the given year.\n",
    "    E.g., Dec2018-Jan2019 belongs to 2018, not 2019.\n",
    "    \n",
    "    Args:\n",
    "        csv_urls: List of CSV URLs from scrape_tfl_usage_csvs()\n",
    "        year: The year to filter by (e.g., 2019)\n",
    "        \n",
    "    Returns:\n",
    "        List of URLs matching the specified year\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r\"JourneyDataExtract(\\d{2}[A-Za-z]{3}\\d{4})-\")\n",
    "    filtered = []\n",
    "    \n",
    "    for url in csv_urls:\n",
    "        filename = url.split(\"/\")[-1]\n",
    "        match = pattern.search(filename)\n",
    "        \n",
    "        if not match:\n",
    "            continue\n",
    "            \n",
    "        start_date_str = match.group(1)\n",
    "        try:\n",
    "            start_date = datetime.strptime(start_date_str, \"%d%b%Y\")\n",
    "            if start_date.year == year:\n",
    "                filtered.append(url)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d16477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_csv(url: str, headers: dict) -> tuple[str, pd.DataFrame | None]:\n",
    "    \"\"\"Download a single CSV and return (filename, dataframe).\"\"\"\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        df = pd.read_csv(StringIO(response.text))\n",
    "        return filename, df\n",
    "    except Exception as e:\n",
    "        return filename, None\n",
    "\n",
    "\n",
    "def download_year_to_parquet_parallel(\n",
    "    year: int, \n",
    "    csv_urls: list[str], \n",
    "    output_dir: str = \"data\",\n",
    "    max_workers: int = 10\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Download all CSVs for a given year in parallel and save as a single parquet file.\n",
    "    \n",
    "    Args:\n",
    "        year: The year to download (e.g., 2019)\n",
    "        csv_urls: List of CSV URLs from scrape_tfl_usage_csvs()\n",
    "        output_dir: Directory to save the parquet file\n",
    "        max_workers: Number of parallel downloads (default 10)\n",
    "        \n",
    "    Returns:\n",
    "        Path to the saved parquet file\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Filter URLs first\n",
    "    year_urls = filter_urls_by_year(csv_urls, year)\n",
    "    \n",
    "    if not year_urls:\n",
    "        raise ValueError(f\"No files found for year {year}\")\n",
    "    \n",
    "    print(f\"Found {len(year_urls)} files for {year}. Downloading with {max_workers} threads...\")\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    dfs = []\n",
    "    failed = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(download_csv, url, headers): url for url in year_urls}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            filename, df = future.result()\n",
    "            if df is not None:\n",
    "                dfs.append(df)\n",
    "                print(f\"{filename} appended\")\n",
    "            else:\n",
    "                failed.append(filename)\n",
    "                print(f\" failed {filename}\")\n",
    "    \n",
    "    if failed:\n",
    "        print(f\"\\nFailed to download {len(failed)} files: {failed}\")\n",
    "    \n",
    "    if not dfs:\n",
    "        raise ValueError(f\"Failed to download any files for year {year}\")\n",
    "    \n",
    "    result = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Convert date columns if they exist\n",
    "    date_cols = [\"Start Date\", \"End Date\"]\n",
    "    for col in date_cols:\n",
    "        if col in result.columns:\n",
    "            result[col] = pd.to_datetime(result[col], dayfirst=True)\n",
    "    \n",
    "    parquet_path = output_path / f\"journeys_{year}.parquet\"\n",
    "    result.to_parquet(parquet_path, index=False)\n",
    "    print(f\"\\nSaved {len(result):,} rows from {len(dfs)} files to {parquet_path}\")\n",
    "    \n",
    "    return parquet_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ad4860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get all the CSV URLs\n",
    "csv_urls = scrape_tfl_usage_csvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c080d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then download a specific year\n",
    "\n",
    "# download_year_to_parquet(2019, csv_urls) # (single thread took 4m 20.2s)\n",
    "# download_year_to_parquet(2020, csv_urls) # (single-thread took 5m 53.7s)\n",
    "# download_year_to_parquet_parallel(2021, csv_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a942c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine all years into one parquet\n",
    "df_2019 = pd.read_parquet(\"data/journeys_2019.parquet\")\n",
    "df_2020 = pd.read_parquet(\"data/journeys_2020.parquet\")\n",
    "df_2021 = pd.read_parquet(\"data/journeys_2021.parquet\")\n",
    "\n",
    "df = pd.concat([df_2019, df_2020, df_2021], ignore_index=True)\n",
    "\n",
    "print(f\"Combined: {len(df):,} rows\")\n",
    "print(f\"2019: {len(df_2019):,} | 2020: {len(df_2020):,} | 2021: {len(df_2021):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06de6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimise memory usage\n",
    "\n",
    "# df = df.drop(columns=['Rental Id'])\n",
    "# # remove records where date is 2022\n",
    "# df = df[df['Start Date'].dt.year != 2022]\n",
    "\n",
    "# # convert duration to mins\n",
    "# df['Duration'] = (df['Duration'] / 60).astype('float32')\n",
    "\n",
    "# # add DoW column\n",
    "# df['DayOfWeek'] = df['Start Date'].dt.day_name().astype('category')\n",
    "\n",
    "# df['Date'] = (df['Start Date'].dt.date).astype('datetime64[ns]')\n",
    "# df['Year'] = (df['Start Date'].dt.year).astype('category') # only 3 years so category is fine\n",
    "\n",
    "# df['Start Hour'] = (df['Start Date'].dt.hour).astype('uint8')\n",
    "\n",
    "# # # Convert station names to category (big savings for repeated strings)\n",
    "# df['EndStation Name'] = df['EndStation Name'].astype('category')\n",
    "# df['StartStation Name'] = df['StartStation Name'].astype('category')\n",
    "# df['Bike Id'] = df['Bike Id'].astype('category')\n",
    "# df['EndStation Id'] = df['EndStation Id'].astype('category')\n",
    "# df['StartStation Id'] = df['StartStation Id'].astype('category')\n",
    "\n",
    "# df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df36409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save combined df as parquet for easier read\n",
    "# df.to_parquet(\"data/journeys_2019_2020_2021.parquet\", engine=\"pyarrow\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b257098f",
   "metadata": {},
   "source": [
    "# EDA on full dataset\n",
    "Summary\n",
    "1. Remove records where date not within 2019-2021\n",
    "2. Prove non-normality of data\n",
    "3. Remove outliers based on threshold of 24 hours\n",
    "4. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61438f94",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4908ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/journeys_2019_2020_2021.parquet\")\n",
    "cat_cols = ['Bike Id', 'EndStation Id', 'StartStation Id', 'Year']\n",
    "for c in cat_cols:\n",
    "    df[c] = df[c].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a300533",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8344f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03da0196",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c05a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# are there any missing values?\n",
    "df[['Start Date', 'End Date', 'Duration', 'Bike Id', 'StartStation Id', 'EndStation Id']].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6af9a1",
   "metadata": {},
   "source": [
    "## Is the data normally distributed?\n",
    "Answer: no, find alternatives to z score outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b536365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 31M rows is too large a sample for plotting, so take a random sample of 1M rows\n",
    "\n",
    "# Take stratified random sample across years\n",
    "df['Year'] = df['Start Date'].dt.year\n",
    "sample = df.groupby('Year', group_keys=False).apply(\n",
    "    lambda x: x.sample(n=min(len(x), 100_000), random_state=42)\n",
    ")\n",
    "print(f\"Sample size: {len(sample):,} rows\")\n",
    "print(f\"Sample by year:\\n{sample['Year'].value_counts().sort_index()}\")\n",
    "\n",
    "# Skewness and Kurtosis\n",
    "skewness = sample['Duration'].skew()\n",
    "kurtosis = sample['Duration'].kurtosis()\n",
    "\n",
    "print(f\"\\n--- Normality Indicators ---\")\n",
    "print(f\"Skewness: {skewness:.2f}  (normal ≈ 0)\")\n",
    "print(f\"Kurtosis: {kurtosis:.2f}  (normal ≈ 0, scipy uses excess kurtosis)\")\n",
    "\n",
    "# Interpretation\n",
    "if abs(skewness) < 0.5:\n",
    "    print(\"-- Skewness: Approximately symmetric\")\n",
    "elif skewness > 0:\n",
    "    print(\"-- Skewness: Right-skewed (long tail of high values)\")\n",
    "else:\n",
    "    print(\"-- Skewness: Left-skewed (long tail of low values)\")\n",
    "\n",
    "if abs(kurtosis) < 1:\n",
    "    print(\"-- Kurtosis: Similar to normal distribution\")\n",
    "elif kurtosis > 0:\n",
    "    print(\"-- Kurtosis: Heavy tails (more outliers than normal)\")\n",
    "else:\n",
    "    print(\"-- Kurtosis: Light tails (fewer outliers than normal)\")\n",
    "\n",
    "# D'Agostino-Pearson test (on smaller sample for speed)\n",
    "test_sample = sample['Duration'].sample(n=5000, random_state=42)\n",
    "stat, p_value = stats.normaltest(test_sample)\n",
    "print(f\"\\nD'Agostino-Pearson test: statistic={stat:.2f}, p-value={p_value:.2e}\")\n",
    "print(f\"-- {'NOT normally distributed (p < 0.05)' if p_value < 0.05 else 'Could be normal (p >= 0.05)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d9567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram with KDE of the sample data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(sample['Duration'], bins=1000, kde=True, stat='density', color='skyblue')\n",
    "plt.xlim(0, sample['Duration'].quantile(0.99))  # limit x-axis to 99th percentile for clarity\n",
    "plt.title('Histogram and KDE of Journey Durations (Sample)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e894f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQ plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "stats.probplot(sample['Duration'], dist=\"norm\", plot=plt)\n",
    "plt.title('QQ Plot of Journey Durations (Sample)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dcb0e5",
   "metadata": {},
   "source": [
    "This plot suggests most of the data is concentrated at the lower values with some extremely high values, explaining the steep rise on the right. With the histogram and the QQ plot, we can conclude that the data is non-normal. This makes sense given the large variety of individual bike rides and variable and unquantifiable traffic scenarios in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32602b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Duration'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add26b6e",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "- Since the data is not normally distributed, use a reasonable value as a threshold. In this case, assume rentals <24h are valid.\n",
    "- Also the client is interested in short term rentals, so long term rentals are outliers for this use case. Drop these values from further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80a8f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe duration- avg journey time per bike\n",
    "df['Duration'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc1557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outliers in duration? if so where and which bikes? what times?\n",
    "# since the data is not normally distributed, use a reasonable value as a threshold. In this case, assume rentals <24h are valid.\n",
    "# Also the client is interested in short term rentals, so long term rentals are outliers.\n",
    "\n",
    "outliers = df[df['Duration'] >= 1440].sort_values(by='Duration', ascending=False) # 9258 records\n",
    "outliers\n",
    "# count by day of week\n",
    "# outliers['DayOfWeek'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ba6687",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03f8ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outliers = df[~df.index.isin(outliers.index)].copy()\n",
    "df_no_outliers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508ecf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outliers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5801f95d",
   "metadata": {},
   "source": [
    "### Bike\n",
    "1. Number of unique bikes - 17766\n",
    "2. Top 10 most used bike ids - [16151, 16082, 15331, 16011, 15529, 16045, 16379, 16344, 15337]\n",
    "3. How many journeys per bike/per day in each of the years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d0243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# are there bike ids with a lot of usage?\n",
    "print (df_no_outliers['Bike Id'].value_counts().head(10))\n",
    "df_no_outliers['Bike Id'].value_counts().describe()  # 17766 unique bike ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fc1283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many journeys per bike/per day in each of the years?\n",
    "# Calculate journeys per bike per day\n",
    "# df_no_outliers['Date'] = (df_no_outliers['Start Date'].dt.date).astype('datetime64[ns]')\n",
    "# df_no_outliers['Year'] = (df_no_outliers['Start Date'].dt.year).astype('uint8')\n",
    "\n",
    "journeys_per_bike_per_day = df_no_outliers.groupby(\n",
    "    ['Year', 'Bike Id', 'Date'], observed=True\n",
    ").size().reset_index(name='journeys')\n",
    "\n",
    "# Summary stats by year\n",
    "summary = journeys_per_bike_per_day.groupby('Year')['journeys'].agg(\n",
    "    ['mean', 'median', 'std', 'min', 'max', 'count']\n",
    ").round(2)\n",
    "summary.columns = ['Avg Journeys/Bike/Day', 'Median', 'Std Dev', 'Min', 'Max', 'Total Bike-Days']\n",
    "print(\"Journeys per bike per day by year:\")\n",
    "print(summary)\n",
    "\n",
    "# Distribution visualization\n",
    "fig = go.Figure()\n",
    "\n",
    "for year in sorted(journeys_per_bike_per_day['Year'].unique()):\n",
    "    year_data = journeys_per_bike_per_day[journeys_per_bike_per_day['Year'] == year]['journeys']\n",
    "    fig.add_trace(go.Box(\n",
    "        y=year_data,\n",
    "        name=str(year),\n",
    "        boxmean=True  # shows mean as dashed line\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='<b>Distribution of Daily Journeys per Bike by Year</b>',\n",
    "    yaxis_title='Journeys per Bike per Day',\n",
    "    xaxis_title='Year',\n",
    "    height=500,\n",
    "    width=700\n",
    ")\n",
    "fig.show()\n",
    "# fig.write_html(\"journeys_per_bike_per_day_by_year.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea5945b",
   "metadata": {},
   "source": [
    "### Time\n",
    "1. Busiest day- Saturday, but not much variation between other days, maximum 11%\n",
    "2. Busiest times- 7-9am during morning rush, 3-8pm during afternoon/evening rush. This indicates a clear commuter pattern. Bikes need to be redistributed before 7am and low overnight usage is an opportunity for maintenance window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276adc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# busiest day/time?\n",
    "df_no_outliers['DayOfWeek'].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b84782d",
   "metadata": {},
   "source": [
    "Above indicates fairly consistent use of bikes on all days of the week, with only a 11% variation on the busiest and quietest days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb92c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Are there any time of day patterns?\n",
    "\n",
    "# # Extract hour from start and end times\n",
    "# df_no_outliers['Start Hour'] = df_no_outliers['Start Date'].dt.hour\n",
    "# df_no_outliers['End Hour'] = df_no_outliers['End Date'].dt.hour\n",
    "\n",
    "# # Count journeys by hour\n",
    "# start_counts = df_no_outliers['Start Hour'].value_counts().sort_index().reset_index()\n",
    "# start_counts.columns = ['hour', 'count']\n",
    "\n",
    "# end_counts = df_no_outliers['End Hour'].value_counts().sort_index().reset_index()\n",
    "# end_counts.columns = ['hour', 'count']\n",
    "\n",
    "# # Define hour mapping for clock face\n",
    "# hour_mapping = {0: 0, 1: 30, 2: 60, 3: 90, 4: 120, 5: 150, 6: 180,\n",
    "#                 7: 210, 8: 240, 9: 270, 10: 300, 11: 330, 12: 0,\n",
    "#                 13: 30, 14: 60, 15: 90, 16: 120, 17: 150, 18: 180,\n",
    "#                 19: 210, 20: 240, 21: 270, 22: 300, 23: 330}\n",
    "\n",
    "# start_counts['theta'] = start_counts['hour'].map(hour_mapping)\n",
    "# end_counts['theta'] = end_counts['hour'].map(hour_mapping)\n",
    "\n",
    "# # Colors for AM/PM\n",
    "# color_map = {\n",
    "#     \"start_am\": \"#88D8B0\",  # Green\n",
    "#     \"start_pm\": \"#40B8E1\",  # Blue\n",
    "#     \"end_am\": \"#F271A7\",    # Pink\n",
    "#     \"end_pm\": \"#FF9F80\"     # Orange\n",
    "# }\n",
    "\n",
    "# fig = make_subplots(\n",
    "#     rows=1, cols=2,\n",
    "#     specs=[[{'type': 'polar'}, {'type': 'polar'}]],\n",
    "#     subplot_titles=[\"Journey Starts\", \"Journey Ends\"]\n",
    "# )\n",
    "\n",
    "# # Journey Starts - AM & PM\n",
    "# for is_am in [True, False]:\n",
    "#     filtered = start_counts[(start_counts['hour'] < 12) if is_am else (start_counts['hour'] >= 12)]\n",
    "#     fig.add_trace(\n",
    "#         go.Barpolar(\n",
    "#             r=filtered['count'],\n",
    "#             theta=filtered['theta'],\n",
    "#             width=30,\n",
    "#             name=\"AM\" if is_am else \"PM\",\n",
    "#             marker_color=color_map[\"start_am\"] if is_am else color_map[\"start_pm\"],\n",
    "#             text=[f\"{h}:00 - {c:,} trips\" for h, c in zip(filtered['hour'], filtered['count'])],\n",
    "#             hoverinfo=\"text\"\n",
    "#         ),\n",
    "#         row=1, col=1\n",
    "#     )\n",
    "\n",
    "# # Journey Ends - AM & PM\n",
    "# for is_am in [True, False]:\n",
    "#     filtered = end_counts[(end_counts['hour'] < 12) if is_am else (end_counts['hour'] >= 12)]\n",
    "#     fig.add_trace(\n",
    "#         go.Barpolar(\n",
    "#             r=filtered['count'],\n",
    "#             theta=filtered['theta'],\n",
    "#             width=30,\n",
    "#             name=\"AM\" if is_am else \"PM\",\n",
    "#             marker_color=color_map[\"end_am\"] if is_am else color_map[\"end_pm\"],\n",
    "#             text=[f\"{h}:00 - {c:,} trips\" for h, c in zip(filtered['hour'], filtered['count'])],\n",
    "#             hoverinfo=\"text\"\n",
    "#         ),\n",
    "#         row=1, col=2\n",
    "#     )\n",
    "\n",
    "# # Layout\n",
    "# polar_axis = dict(\n",
    "#     radialaxis=dict(visible=False),\n",
    "#     angularaxis=dict(\n",
    "#         tickmode=\"array\",\n",
    "#         tickvals=[0, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330],\n",
    "#         ticktext=[\"12\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\"],\n",
    "#         direction=\"clockwise\",\n",
    "#         rotation=90\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# fig.update_layout(\n",
    "#     height=600,\n",
    "#     width=1000,\n",
    "#     title=\"Hourly Bike Usage Patterns\",\n",
    "#     polar=polar_axis,\n",
    "#     polar2=polar_axis,\n",
    "#     showlegend=False\n",
    "# )\n",
    "\n",
    "# # Custom legend\n",
    "# legend_items = [\n",
    "#     (\"Starts AM\", color_map[\"start_am\"]),\n",
    "#     (\"Starts PM\", color_map[\"start_pm\"]),\n",
    "#     (\"Ends AM\", color_map[\"end_am\"]),\n",
    "#     (\"Ends PM\", color_map[\"end_pm\"]),\n",
    "# ]\n",
    "# for idx, (label, color) in enumerate(legend_items):\n",
    "#     fig.add_annotation(\n",
    "#         x=1.08, y=0.9 - (idx * 0.08),\n",
    "#         xref='paper', yref='paper',\n",
    "#         text=f\"<b>{label}</b>\",\n",
    "#         showarrow=False,\n",
    "#         font=dict(color=color, size=12)\n",
    "#     )\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3425e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig.write_html(\"hourly bike usage.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fcd75",
   "metadata": {},
   "source": [
    "The most number of journeys in the morning are between 7-9am and 4-8pm indicating a clear commuter pattern. Bikes need to be redistributed before 7am and low overnight usage is an opportunity for maintenance window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca687b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count journeys by start hour\n",
    "# df_no_outliers['Start Hour'] = df_no_outliers['Start Date'].dt.hour\n",
    "hourly = df_no_outliers.groupby('Start Hour').size().reset_index()\n",
    "hourly.columns = ['hour', 'count']\n",
    "\n",
    "# Define rush hours\n",
    "rush_hours = list(range(7, 10)) + list(range(15, 21))  # 7-9AM and 3-8PM\n",
    "hourly['is_rush'] = hourly['hour'].isin(rush_hours)\n",
    "hourly['color'] = hourly['is_rush'].map({True: '#2C3E50', False: '#88D8B0'})\n",
    "\n",
    "# Format labels (show as millions with 1 decimal)\n",
    "hourly['label'] = hourly['count'].apply(lambda x: f'{x/1e6:.2f}M')\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=hourly['hour'],\n",
    "    y=hourly['count'],\n",
    "    marker_color=hourly['color'],\n",
    "    text=hourly['label'],\n",
    "    textposition='outside',\n",
    "    textangle=-25,\n",
    "    textfont=dict(size=10),\n",
    "    hovertemplate='%{x}:00<br>%{y:,.0f} journeys<extra></extra>'\n",
    "))\n",
    "\n",
    "# Layout\n",
    "fig.update_layout(\n",
    "    title=dict(\n",
    "        text='<b>When Do Londoners Use Bikes?</b><br><sup>Rush hours highlighted (7-9AM & 3-8PM)</sup>',\n",
    "        x=0.5\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title='Hour of Day',\n",
    "        tickmode='array',\n",
    "        tickvals=list(range(24)),\n",
    "        ticktext=[f'{h:02d}:00' for h in range(24)],\n",
    "        tickangle=-45\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Number of Journeys',\n",
    "        tickformat=',.0f'\n",
    "    ),\n",
    "    height=650,\n",
    "    width=1000,\n",
    "    showlegend=False,\n",
    "    margin=dict(t=100, b=80),\n",
    "    \n",
    "    # Add annotation for insight\n",
    "    annotations=[\n",
    "        dict(\n",
    "            x=0.01, y=0.98,\n",
    "            xref='paper', yref='paper',\n",
    "            text='<b>Key Insight:</b><br>Peak at 8AM & 6PM<br>→ Strong commuter pattern',\n",
    "            showarrow=False,\n",
    "            align='left',\n",
    "            bgcolor='rgba(255,248,220,0.8)',\n",
    "            bordercolor='#888',\n",
    "            borderwidth=1,\n",
    "            borderpad=6,\n",
    "            font=dict(size=11)\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "# fig.write_html(\"hourly_usage_bars.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b2f79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outliers.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c369c9",
   "metadata": {},
   "source": [
    "### Duration/usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278b6daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average journey duration by start and end stations\n",
    "avg_duration_start = df_no_outliers.groupby('StartStation Name', observed=True)['Duration'].mean().sort_values(ascending=False).head(10)\n",
    "avg_duration_end = df_no_outliers.groupby('EndStation Name', observed=True)['Duration'].mean().sort_values(ascending=False).head(10)\n",
    "print(\"Top 10 Start Stations by Average Duration:\")\n",
    "print(avg_duration_start)\n",
    "print(\"Top 10 End Stations by Average Duration:\")\n",
    "print(avg_duration_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b90c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many journeys over and under 30 mins?\n",
    "under_30mins = df_no_outliers[df_no_outliers['Duration'] <= 30]\n",
    "over_30mins = df_no_outliers[(df_no_outliers['Duration'] > 30) & (df_no_outliers['Duration'] < 60)] # but under 60 mins\n",
    "print(f\"Journeys <= 30 mins: {len(under_30mins):,} ({len(under_30mins) / len(df_no_outliers) * 100:.2f}%)\")\n",
    "print(f\"Journeys > 30 mins <=60 mins: {len(over_30mins):,} ({len(over_30mins) / len(df_no_outliers) * 100:.2f}%)\")\n",
    "# 4.63% are over 60 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3083b5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekday vs weekend duration and number of journeys\n",
    "# Create weekend flag\n",
    "df_no_outliers['IsWeekend'] = df_no_outliers['DayOfWeek'].isin(['Saturday', 'Sunday'])\n",
    "\n",
    "# Calculate AVERAGE journeys per day (normalized metric)\n",
    "daily_counts = df_no_outliers.groupby(['Date', 'IsWeekend']).size().reset_index(name='journeys')\n",
    "avg_per_day = daily_counts.groupby('IsWeekend')['journeys'].mean().reset_index()\n",
    "avg_per_day['Day Type'] = avg_per_day['IsWeekend'].map({False: 'Weekday', True: 'Weekend'})\n",
    "\n",
    "# Calculate average duration\n",
    "avg_duration = df_no_outliers.groupby('IsWeekend')['Duration'].mean().reset_index()\n",
    "avg_duration['Day Type'] = avg_duration['IsWeekend'].map({False: 'Weekday', True: 'Weekend'})\n",
    "\n",
    "print(\"=== Weekday vs Weekend Summary ===\")\n",
    "print(f\"\\nAverage Journeys Per Day:\")\n",
    "for _, row in avg_per_day.iterrows():\n",
    "    print(f\"  {row['Day Type']}: {row['journeys']:,.0f}\")\n",
    "\n",
    "print(f\"\\nAverage Duration (mins):\")\n",
    "for _, row in avg_duration.iterrows():\n",
    "    print(f\"  {row['Day Type']}: {row['Duration']:.1f}\")\n",
    "\n",
    "# Create side-by-side visualization\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=['<b>Avg Journeys per Day</b>', '<b>Avg Duration (mins)</b>']\n",
    ")\n",
    "\n",
    "colors = ['#2C3E50', '#88D8B0']  # Weekday dark, Weekend light\n",
    "\n",
    "# Bar 1: Average journeys per day\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=avg_per_day['Day Type'],\n",
    "        y=avg_per_day['journeys'],\n",
    "        marker_color=colors,\n",
    "        text=avg_per_day['journeys'].apply(lambda x: f'{x:,.0f}'),\n",
    "        textposition='outside',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Bar 2: Average duration\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=avg_duration['Day Type'],\n",
    "        y=avg_duration['Duration'],\n",
    "        marker_color=colors,\n",
    "        text=avg_duration['Duration'].apply(lambda x: f'{x:.1f}'),\n",
    "        textposition='outside',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='<b>Weekday vs Weekend Usage Patterns</b>',\n",
    "    height=450,\n",
    "    width=800,\n",
    "    margin=dict(t=80)\n",
    ")\n",
    "\n",
    "fig.update_yaxes(title_text='Journeys', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Minutes', row=1, col=2)\n",
    "\n",
    "fig.show()\n",
    "# fig.write_html(\"weekday_vs_weekend_summary.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383f7600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal Analysis\n",
    "# Define seasons based on month\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Autumn'\n",
    "\n",
    "df_no_outliers['Month'] = df_no_outliers['Start Date'].dt.month\n",
    "df_no_outliers['Season'] = df_no_outliers['Month'].apply(get_season)\n",
    "\n",
    "# Calculate average journeys per day by season\n",
    "daily_by_season = df_no_outliers.groupby(['Date', 'Season']).size().reset_index(name='journeys')\n",
    "avg_journeys = daily_by_season.groupby('Season')['journeys'].mean().reset_index()\n",
    "\n",
    "# Calculate average duration by season\n",
    "avg_duration = df_no_outliers.groupby('Season')['Duration'].mean().reset_index()\n",
    "\n",
    "# Merge for plotting\n",
    "seasonal = avg_journeys.merge(avg_duration, on='Season')\n",
    "seasonal.columns = ['Season', 'Avg Journeys/Day', 'Avg Duration (mins)']\n",
    "\n",
    "# Order seasons logically\n",
    "season_order = ['Spring', 'Summer', 'Autumn', 'Winter']\n",
    "seasonal['Season'] = pd.Categorical(seasonal['Season'], categories=season_order, ordered=True)\n",
    "seasonal = seasonal.sort_values('Season')\n",
    "\n",
    "print(\"=== Seasonal Summary ===\")\n",
    "print(seasonal.to_string(index=False))\n",
    "\n",
    "# Create visualization\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=['<b>Avg Journeys per Day</b>', '<b>Avg Duration (mins)</b>']\n",
    ")\n",
    "\n",
    "colors = ['#88D8B0', '#FF6B6B', '#FFE66D', '#4ECDC4']  # Spring, Summer, Autumn, Winter\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=seasonal['Season'],\n",
    "        y=seasonal['Avg Journeys/Day'],\n",
    "        marker_color=colors,\n",
    "        text=seasonal['Avg Journeys/Day'].apply(lambda x: f'{x:,.0f}'),\n",
    "        textposition='outside',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=seasonal['Season'],\n",
    "        y=seasonal['Avg Duration (mins)'],\n",
    "        marker_color=colors,\n",
    "        text=seasonal['Avg Duration (mins)'].apply(lambda x: f'{x:.1f}'),\n",
    "        textposition='outside',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='<b>Seasonal Usage Patterns</b>',\n",
    "    height=450,\n",
    "    width=900,\n",
    "    margin=dict(t=80)\n",
    ")\n",
    "\n",
    "fig.update_yaxes(title_text='Journeys/Day', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Minutes', row=1, col=2)\n",
    "\n",
    "fig.show()\n",
    "# fig.write_html(\"seasonal_usage_summary.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17e1771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yearly Analysis - Avg Journeys/Day and Avg Duration\n",
    "# Calculate average journeys per day by year\n",
    "daily_by_year = df_no_outliers.groupby(['Date', 'Year']).size().reset_index(name='journeys')\n",
    "avg_journeys_year = daily_by_year.groupby('Year')['journeys'].mean().reset_index()\n",
    "\n",
    "# Calculate average duration by year\n",
    "avg_duration_year = df_no_outliers.groupby('Year')['Duration'].mean().reset_index()\n",
    "\n",
    "# Merge for plotting\n",
    "yearly = avg_journeys_year.merge(avg_duration_year, on='Year')\n",
    "yearly.columns = ['Year', 'Avg Journeys/Day', 'Avg Duration (mins)']\n",
    "\n",
    "print(\"=== Yearly Summary ===\")\n",
    "print(yearly.to_string(index=False))\n",
    "\n",
    "# Create visualization\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=['<b>Avg Journeys per Day</b>', '<b>Avg Duration (mins)</b>']\n",
    ")\n",
    "\n",
    "colors = ['#2C3E50', '#E74C3C', '#3498DB']  # 2019, 2020, 2021\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=yearly['Year'].astype(str),\n",
    "        y=yearly['Avg Journeys/Day'],\n",
    "        marker_color=colors,\n",
    "        text=yearly['Avg Journeys/Day'].apply(lambda x: f'{x:,.0f}'),\n",
    "        textposition='outside',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=yearly['Year'].astype(str),\n",
    "        y=yearly['Avg Duration (mins)'],\n",
    "        marker_color=colors,\n",
    "        text=yearly['Avg Duration (mins)'].apply(lambda x: f'{x:.1f}'),\n",
    "        textposition='outside',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='<b>Yearly Usage Patterns (2019-2021)</b>',\n",
    "    height=450,\n",
    "    width=800,\n",
    "    margin=dict(t=80)\n",
    ")\n",
    "\n",
    "fig.update_yaxes(title_text='Journeys/Day', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Minutes', row=1, col=2)\n",
    "\n",
    "fig.show()\n",
    "# fig.write_html(\"yearly_usage_summary.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb9d1d",
   "metadata": {},
   "source": [
    "2020: Likely lower journeys/day (COVID lockdowns), but possibly longer durations (leisure rides during exercise allowance, fewer short commutes) but surprisingly there was only a 0.64% decrease from 2019.\n",
    "2021: Recovery- 5.5% increase since 2020\n",
    "Longer durations in 2020-21 may indicate shift from commuting to recreational use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f890ad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_no_outliers.drop(columns=['IsWeekend', 'Month', 'Season'], inplace=True)\n",
    "df_no_outliers.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c2db97",
   "metadata": {},
   "source": [
    "### Location analyses\n",
    "- chlropleth map of start and end locations\n",
    "- how many journeys started and ended at the same location?\n",
    "- most popular start and end stations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8e4a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outliers['StartStation Id'].nunique(), df_no_outliers['EndStation Id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff32c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get station coordinates from TfL API\n",
    "\n",
    "# TfL BikePoint API (free, no key needed)\n",
    "url = \"https://api.tfl.gov.uk/BikePoint\"\n",
    "response = requests.get(url)\n",
    "stations = response.json()\n",
    "\n",
    "# Create lookup: station_id -> (lat, lon, name)\n",
    "station_coords = {}\n",
    "for s in stations:\n",
    "    # Extract ID number from \"BikePoints_123\" format\n",
    "    station_id = int(s['id'].replace('BikePoints_', ''))\n",
    "    station_coords[station_id] = {\n",
    "        'lat': s['lat'],\n",
    "        'lon': s['lon'],\n",
    "        'name': s['commonName']\n",
    "    }\n",
    "\n",
    "print(f\"Loaded {len(station_coords)} stations with coordinates\")\n",
    "\n",
    "stations_df = (\n",
    "    pd.DataFrame.from_dict(station_coords, orient='index')\n",
    "    .rename_axis('StationId')\n",
    "    .reset_index()\n",
    "    [['StationId', 'lat', 'lon']]\n",
    ")\n",
    "\n",
    "stations_df['lat'] = stations_df['lat'].astype('float32')\n",
    "stations_df['lon'] = stations_df['lon'].astype('float32')\n",
    "stations_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a143b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store stations_df for future use\n",
    "stations_df.to_parquet(\"data/station_coordinates.parquet\", engine=\"pyarrow\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61aabd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = pd.read_parquet(\"data/station_coordinates.parquet\")\n",
    "stations.info()\n",
    "stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fa8f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations['StationId'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89745bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outliers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6479de",
   "metadata": {},
   "source": [
    "### Station imbalance \n",
    "1. Which stations are are net sources vs net sinks of bikes - critical for operational planning:\n",
    "- Red circles = More journeys END here (bikes accumulate)\n",
    "- Green circles = More journeys START here (bikes get depleted)\n",
    "- Size = magnitude of imbalance\n",
    "- This directly addresses supply chain management concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f36d327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate station imbalance (ends - starts)\n",
    "starts = df_no_outliers.groupby('StartStation Id', observed=True).size()\n",
    "ends = df_no_outliers.groupby('EndStation Id', observed=True).size()\n",
    "\n",
    "# Merge to get both metrics\n",
    "station_balance = pd.DataFrame({\n",
    "    'starts': starts,\n",
    "    'ends': ends\n",
    "}).fillna(0).astype(int)\n",
    "\n",
    "station_balance['net_imbalance'] = station_balance['ends'] - station_balance['starts']\n",
    "station_balance['total_volume'] = station_balance['starts'] + station_balance['ends']\n",
    "station_balance['imbalance_pct'] = (station_balance['net_imbalance'] / station_balance['total_volume'] * 100).round(1)\n",
    "\n",
    "# Add station names\n",
    "station_names = df_no_outliers.groupby('StartStation Id', observed=True)['StartStation Name'].first()\n",
    "station_balance = station_balance.merge(\n",
    "    station_names.rename('name'), \n",
    "    left_index=True, \n",
    "    right_index=True, \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Total stations: {len(station_balance)}\")\n",
    "print(f\"\\nTop 5 Sinks (bikes accumulate):\")\n",
    "print(station_balance.nlargest(5, 'net_imbalance')[['name', 'starts', 'ends', 'net_imbalance', 'imbalance_pct']])\n",
    "print(f\"\\nTop 5 Sources (bikes depleted):\")\n",
    "print(station_balance.nsmallest(5, 'net_imbalance')[['name', 'starts', 'ends', 'net_imbalance', 'imbalance_pct']])\n",
    "\n",
    "# Visualization: Horizontal Bar Chart\n",
    "top_sinks = station_balance.nlargest(10, 'net_imbalance').sort_values('net_imbalance')\n",
    "top_sources = station_balance.nsmallest(10, 'net_imbalance').sort_values('net_imbalance', ascending=False)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=['<b>Top 10 Sources</b><br>(Bikes Depleted)', '<b>Top 10 Sinks</b><br>(Bikes Accumulate)'],\n",
    "    horizontal_spacing=0.25\n",
    ")\n",
    "\n",
    "# Sources (left, negative values)\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        y=top_sources['name'],\n",
    "        x=top_sources['net_imbalance'],\n",
    "        orientation='h',\n",
    "        marker_color='#E74C3C',\n",
    "        text=top_sources['net_imbalance'].apply(lambda x: f'{x:,}'),\n",
    "        textposition='outside',\n",
    "        textfont=dict(size=10),\n",
    "        hovertemplate='<b>%{y}</b><br>Net: %{x:,} bikes<br>Starts: ' + \n",
    "                      top_sources['starts'].astype(str) + '<br>Ends: ' + \n",
    "                      top_sources['ends'].astype(str) + '<extra></extra>',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Sinks (right, positive values)\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        y=top_sinks['name'],\n",
    "        x=top_sinks['net_imbalance'],\n",
    "        orientation='h',\n",
    "        marker_color='#27AE60',\n",
    "        text=top_sinks['net_imbalance'].apply(lambda x: f'+{x:,}'),\n",
    "        textposition='outside',\n",
    "        textfont=dict(size=10),\n",
    "        hovertemplate='<b>%{y}</b><br>Net: +%{x:,} bikes<br>Starts: ' + \n",
    "                      top_sinks['starts'].astype(str) + '<br>Ends: ' + \n",
    "                      top_sinks['ends'].astype(str) + '<extra></extra>',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(\n",
    "        text='<b>Station Imbalance: Where to Redistribute Bikes</b><br><sup>Based on 31.4M journeys (2019-2021)</sup>',\n",
    "        x=0.5\n",
    "    ),\n",
    "    height=700,\n",
    "    width=1500,\n",
    "    margin=dict(t=50, l=200, r=50)\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text='Net Imbalance (bikes)', row=1, col=1)\n",
    "fig.update_xaxes(title_text='Net Imbalance (bikes)', row=1, col=2)\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=0.5, y=-0.12,\n",
    "    xref='paper', yref='paper',\n",
    "    text='<b>Action:</b> Sources need morning restocking | Sinks need evening collection',\n",
    "    showarrow=False,\n",
    "    bgcolor='rgba(255,248,220,0.8)',\n",
    "    bordercolor='#888',\n",
    "    borderwidth=1,\n",
    "    borderpad=6,\n",
    "    font=dict(size=11)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_html(\"station_imbalance_bars.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1f5011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge station_balance with coordinates\n",
    "\n",
    "station_balance_geo = station_balance.reset_index().merge(\n",
    "    stations,\n",
    "    left_index=True,\n",
    "    right_on='StationId',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Create map centered on London\n",
    "london_map = folium.Map(\n",
    "    location=[51.5074, -0.1278],\n",
    "    zoom_start=12,\n",
    "    tiles='cartodbpositron'\n",
    ")\n",
    "\n",
    "# Add markers for top 20 imbalanced stations (10 sources + 10 sinks)\n",
    "top_20_imbalanced = pd.concat([\n",
    "    station_balance_geo.nlargest(10, 'net_imbalance'),\n",
    "    station_balance_geo.nsmallest(10, 'net_imbalance')\n",
    "])\n",
    "\n",
    "for _, row in top_20_imbalanced.iterrows():\n",
    "    is_sink = row['net_imbalance'] > 0\n",
    "    color = 'green' if is_sink else 'red'\n",
    "    icon_symbol = 'arrow-up' if is_sink else 'arrow-down'\n",
    "    \n",
    "    # Marker size based on magnitude\n",
    "    radius = min(abs(row['net_imbalance']) / 100, 20)  # Cap at 20\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['lon']],\n",
    "        radius=radius,\n",
    "        popup=folium.Popup(\n",
    "            f\"\"\"<b>{row['name']}</b><br>\n",
    "            Net: {row['net_imbalance']:+,} bikes<br>\n",
    "            Starts: {row['starts']:,}<br>\n",
    "            Ends: {row['ends']:,}<br>\n",
    "            Type: {'SINK (accumulates)' if is_sink else 'SOURCE (depleted)'}\"\"\",\n",
    "            max_width=250\n",
    "        ),\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fillColor=color,\n",
    "        fillOpacity=0.6,\n",
    "        weight=2\n",
    "    ).add_to(london_map)\n",
    "\n",
    "# Add legend\n",
    "legend_html = '''\n",
    "<div style=\"position: fixed; \n",
    "     top: 10px; right: 10px; width: 180px; \n",
    "     background-color: white; border:2px solid grey; z-index:9999; \n",
    "     font-size:14px; padding: 10px\">\n",
    "     <p><b>Station Imbalance</b></p>\n",
    "     <p><span style=\"color:green;\">●</span> Sinks (bikes accumulate)</p>\n",
    "     <p><span style=\"color:red;\">●</span> Sources (bikes depleted)</p>\n",
    "     <p style=\"font-size:11px; margin-top:10px;\">Circle size = magnitude</p>\n",
    "</div>\n",
    "'''\n",
    "london_map.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "london_map.save('station_imbalance_map.html')\n",
    "print(\"Map saved to station_imbalance_map.html\")\n",
    "london_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ff4c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge station_balance (which has total_volume) with coordinates\n",
    "station_popularity_geo = station_balance_geo.copy()\n",
    "# station_popularity_geo = station_balance.merge(\n",
    "#     stations,\n",
    "#     left_index=True,\n",
    "#     right_on='StationId',\n",
    "#     how='inner'\n",
    "# )\n",
    "\n",
    "# Create map centered on London\n",
    "popularity_map = folium.Map(\n",
    "    location=[51.5074, -0.1278],\n",
    "    zoom_start=12,\n",
    "    tiles='cartodbpositron'\n",
    ")\n",
    "\n",
    "# Normalize radius for visualization (scale to reasonable circle sizes)\n",
    "max_volume = station_popularity_geo['total_volume'].max()\n",
    "station_popularity_geo['radius'] = (station_popularity_geo['total_volume'] / max_volume * 25) + 3  # 3-28 range\n",
    "\n",
    "# Add all stations, colored by volume intensity\n",
    "for _, row in station_popularity_geo.iterrows():\n",
    "    # Color intensity based on volume (blue gradient)\n",
    "    intensity = row['total_volume'] / max_volume\n",
    "    color = f'#{int(44 + (1-intensity)*100):02x}{int(62 + (1-intensity)*100):02x}{int(80 + (1-intensity)*80):02x}'\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['lon']],\n",
    "        radius=row['radius'],\n",
    "        popup=folium.Popup(\n",
    "            f\"\"\"<b>{row['name']}</b><br>\n",
    "            Total Journeys: {row['total_volume']:,}<br>\n",
    "            Starts: {row['starts']:,}<br>\n",
    "            Ends: {row['ends']:,}\"\"\",\n",
    "            max_width=250\n",
    "        ),\n",
    "        color='#2C3E50',\n",
    "        fill=True,\n",
    "        fillColor='#3498DB',\n",
    "        fillOpacity=0.4 + (intensity * 0.4),  # 0.4-0.8 opacity\n",
    "        weight=1\n",
    "    ).add_to(popularity_map)\n",
    "\n",
    "# Highlight top 20 busiest stations with labels\n",
    "top_10_popular = station_popularity_geo.nlargest(20, 'total_volume')\n",
    "for _, row in top_10_popular.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['lon']],\n",
    "        radius=row['radius'],\n",
    "        popup=folium.Popup(\n",
    "            f\"\"\"<b>🔥 {row['name']}</b><br>\n",
    "            Total Journeys: {row['total_volume']:,}<br>\n",
    "            Rank: Top 20 Busiest\"\"\",\n",
    "            max_width=250\n",
    "        ),\n",
    "        color='#E74C3C',\n",
    "        fill=True,\n",
    "        fillColor='#E74C3C',\n",
    "        fillOpacity=0.7,\n",
    "        weight=3\n",
    "    ).add_to(popularity_map)\n",
    "\n",
    "# Add legend\n",
    "legend_html = '''\n",
    "<div style=\"position: fixed; \n",
    "     top: 10px; right: 10px; width: 200px; \n",
    "     background-color: white; border:2px solid grey; z-index:9999; \n",
    "     font-size:14px; padding: 10px\">\n",
    "     <p><b>Station Popularity</b></p>\n",
    "     <p><span style=\"color:#E74C3C;\">●</span> Top 10 Busiest</p>\n",
    "     <p><span style=\"color:#3498DB;\">●</span> All Stations</p>\n",
    "     <p style=\"font-size:11px; margin-top:10px;\">Circle size = total journeys<br>(starts + ends)</p>\n",
    "</div>\n",
    "'''\n",
    "popularity_map.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "popularity_map.save('station_popularity_map.html')\n",
    "print(\"Map saved to station_popularity_map.html\")\n",
    "\n",
    "# Print top 10 summary\n",
    "print(\"\\n=== Top 20 Busiest Stations ===\")\n",
    "print(top_10_popular[['name', 'total_volume', 'starts', 'ends']].to_string(index=False))\n",
    "\n",
    "popularity_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3d9c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find journeys where start and end station are the same\n",
    "same_station = df_no_outliers[\n",
    "    df_no_outliers['StartStation Id'] == df_no_outliers['EndStation Id']\n",
    "].copy()\n",
    "\n",
    "total_journeys = len(df_no_outliers)\n",
    "same_station_count = len(same_station)\n",
    "\n",
    "print(f\"=== Same-Station Journeys (Round Trips) ===\")\n",
    "print(f\"Total journeys: {total_journeys:,}\")\n",
    "print(f\"Same-station journeys: {same_station_count:,} ({same_station_count/total_journeys*100:.2f}%)\")\n",
    "print(f\"Different-station journeys: {total_journeys - same_station_count:,} ({(total_journeys - same_station_count)/total_journeys*100:.2f}%)\")\n",
    "\n",
    "# Average duration comparison\n",
    "print(f\"\\n=== Duration Comparison ===\")\n",
    "print(f\"Same-station avg duration: {same_station['Duration'].mean():.1f} mins\")\n",
    "print(f\"Different-station avg duration: {df_no_outliers[df_no_outliers['StartStation Id'] != df_no_outliers['EndStation Id']]['Duration'].mean():.1f} mins\")\n",
    "\n",
    "# Top stations for same-station journeys (likely leisure/parks)\n",
    "same_station_by_station = same_station.groupby(\n",
    "    ['StartStation Id', 'StartStation Name'], observed=True\n",
    ").agg(\n",
    "    round_trips=('Duration', 'count'),\n",
    "    avg_duration=('Duration', 'mean')\n",
    ").reset_index()\n",
    "same_station_by_station.columns = ['StationId', 'name', 'round_trips', 'avg_duration']\n",
    "same_station_by_station = same_station_by_station.sort_values('round_trips', ascending=False)\n",
    "\n",
    "print(f\"\\n=== Top 10 Stations for Round Trips ===\")\n",
    "print(same_station_by_station.head(10)[['name', 'round_trips', 'avg_duration']].to_string(index=False))\n",
    "\n",
    "# Visualization: Bar chart\n",
    "top_round_trip_stations = same_station_by_station.head(15)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    y=top_round_trip_stations['name'],\n",
    "    x=top_round_trip_stations['round_trips'],\n",
    "    orientation='h',\n",
    "    marker_color='#9B59B6',\n",
    "    text=top_round_trip_stations['round_trips'].apply(lambda x: f'{x:,}'),\n",
    "    textposition='outside',\n",
    "    hovertemplate='<b>%{y}</b><br>Round trips: %{x:,}<extra></extra>'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(\n",
    "        text=f'<b>Top 15 Stations for Round Trips</b><br><sup>{same_station_count:,} same-station journeys ({same_station_count/total_journeys*100:.1f}% of total)</sup>',\n",
    "        x=0.5\n",
    "    ),\n",
    "    xaxis_title='Number of Round Trips',\n",
    "    yaxis=dict(autorange='reversed'),\n",
    "    height=600,\n",
    "    width=900,\n",
    "    margin=dict(l=250)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "# fig.write_html(\"same_station_journeys.html\")\n",
    "\n",
    "# Map: Highlight round-trip hotspots\n",
    "round_trip_geo = same_station_by_station.merge(\n",
    "    stations,\n",
    "    on='StationId',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "round_trip_map = folium.Map(\n",
    "    location=[51.5074, -0.1278],\n",
    "    zoom_start=12,\n",
    "    tiles='cartodbpositron'\n",
    ")\n",
    "\n",
    "# Normalize for circle size\n",
    "max_trips = round_trip_geo['round_trips'].max()\n",
    "\n",
    "for _, row in round_trip_geo.nlargest(30, 'round_trips').iterrows():\n",
    "    radius = (row['round_trips'] / max_trips * 20) + 5\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['lon']],\n",
    "        radius=radius,\n",
    "        popup=folium.Popup(\n",
    "            f\"\"\"<b>{row['name']}</b><br>\n",
    "            Round Trips: {row['round_trips']:,}<br>\n",
    "            Avg Duration: {row['avg_duration']:.1f} mins<br>\n",
    "            <i>Likely leisure/park location</i>\"\"\",\n",
    "            max_width=250\n",
    "        ),\n",
    "        color='#9B59B6',\n",
    "        fill=True,\n",
    "        fillColor='#9B59B6',\n",
    "        fillOpacity=0.6,\n",
    "        weight=2\n",
    "    ).add_to(round_trip_map)\n",
    "\n",
    "# Legend\n",
    "legend_html = '''\n",
    "<div style=\"position: fixed; \n",
    "     top: 10px; right: 10px; width: 200px; \n",
    "     background-color: white; border:2px solid grey; z-index:9999; \n",
    "     font-size:14px; padding: 10px\">\n",
    "     <p><b>Round Trip Hotspots</b></p>\n",
    "     <p><span style=\"color:#9B59B6;\">●</span> Same start & end station</p>\n",
    "     <p style=\"font-size:11px; margin-top:10px;\">Circle size = # round trips<br>\n",
    "     Likely parks or leisure areas</p>\n",
    "</div>\n",
    "'''\n",
    "round_trip_map.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "round_trip_map.save('round_trip_hotspots_map.html')\n",
    "print(\"\\nMap saved to round_trip_hotspots_map.html\")\n",
    "round_trip_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6037da15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
